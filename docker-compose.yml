# HIGH-014 FIX: Startup Order Documentation
# ==============================================================================
# CRITICAL: Services must start in the following order (enforced by depends_on):
# 1. postgres-db (foundation - no dependencies)
# 2. minio (storage - no dependencies)
# 3. metrics-collector (depends on postgres-db)
# 4. llm-service (depends on postgres-db)
# 5. embedding-service (depends on postgres-db)
# 6. reverse-proxy (depends on postgres-db, minio, metrics-collector, llm-service, embedding-service)
# 7. dashboard-backend (depends on postgres-db, minio, metrics-collector, llm-service, embedding-service, reverse-proxy)
# 8. dashboard-frontend (depends on reverse-proxy)
# 9. n8n (depends on postgres-db, llm-service, embedding-service, minio)
# 10. self-healing-agent (depends on all services - monitors all)
# 11. backup-service (depends on postgres-db, minio)
# 12. telegram-bot (depends on postgres-db, dashboard-backend)
#
# IMPORTANT: All services implement retry logic for database and service connections.
# Healthchecks are robust (see HIGH-010 fix) and properly handle timeouts.
# ==============================================================================

networks:
  arasul-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.30.0.0/24

volumes:
  arasul-postgres:
  arasul-minio:
  arasul-n8n:
  arasul-llm-models:
  arasul-embeddings-models:
  arasul-metrics:
  arasul-qdrant:
  arasul-logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /arasul/logs
  arasul-letsencrypt:

services:
  # 1. PostgreSQL - Telemetry and Audit Database
  postgres-db:
    image: postgres:16-alpine
    container_name: postgres-db
    hostname: postgres-db
    restart: always
    networks:
      - arasul-net
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_MAX_CONNECTIONS: ${POSTGRES_MAX_CONNECTIONS}
    volumes:
      - arasul-postgres:/var/lib/postgresql/data
      - ./services/postgres/init:/docker-entrypoint-initdb.d
      - ./services/postgres/migrations:/migrations
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 2s
      retries: 3
    deploy:
      resources:
        limits:
          memory: ${RAM_LIMIT_POSTGRES}
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "10"

  # 2. MinIO - Object Storage
  minio:
    image: minio/minio:latest
    container_name: minio
    hostname: minio
    restart: always
    networks:
      - arasul-net
    ports:
      - "9001:9001"  # MinIO Console - Direct access
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_BROWSER: ${MINIO_BROWSER}
    volumes:
      - arasul-minio:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 1s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "10"
    # PHASE2-FIX: Add memory limits
    deploy:
      resources:
        limits:
          memory: 4G

  # 2.5 Qdrant - Vector Database for RAG
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    hostname: qdrant
    restart: always
    networks:
      - arasul-net
    ports:
      - "6333:6333"  # HTTP API
      - "6334:6334"  # gRPC API
    volumes:
      - arasul-qdrant:/qdrant/storage
    environment:
      QDRANT__SERVICE__GRPC_PORT: "6334"
      QDRANT__SERVICE__HTTP_PORT: "6333"
    healthcheck:
      test: ["CMD", "test", "-f", "/qdrant/storage/raft_state.json"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "10"
    # PHASE2-FIX: Add memory limits
    deploy:
      resources:
        limits:
          memory: 4G

  # 3. Metrics Collector
  metrics-collector:
    build:
      context: ./services/metrics-collector
      dockerfile: Dockerfile
    container_name: metrics-collector
    hostname: metrics-collector
    restart: always
    networks:
      - arasul-net
    environment:
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PORT: ${POSTGRES_PORT}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      METRICS_INTERVAL_LIVE: ${METRICS_INTERVAL_LIVE}
      METRICS_INTERVAL_PERSIST: ${METRICS_INTERVAL_PERSIST}
      LOG_LEVEL: ${LOG_LEVEL}
    volumes:
      - /sys:/host/sys:ro
      - /proc:/host/proc:ro
      - arasul-metrics:/cache
    depends_on:
      postgres-db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9100/health"]
      interval: 10s
      timeout: 1s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 512M
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "10"

  # 4. LLM Service (Ollama)
  llm-service:
    build:
      context: ./services/llm-service
      dockerfile: Dockerfile
    container_name: llm-service
    hostname: llm-service
    restart: always
    networks:
      - arasul-net
    environment:
      OLLAMA_HOST: 0.0.0.0:11434
      # OLLAMA_MODELS removed - Ollama uses default /root/.ollama where volume is mounted
      # This ensures models persist across container restarts (see HIGH-017)
      LLM_MODEL: ${LLM_MODEL:-qwen3:14b-q8}
      LLM_KEEP_ALIVE_SECONDS: ${LLM_KEEP_ALIVE_SECONDS:-300}
      # CRITICAL-FIX: Configurable startup timeout (default 120s, increase for slow systems)
      OLLAMA_STARTUP_TIMEOUT: ${OLLAMA_STARTUP_TIMEOUT:-120}
    volumes:
      - arasul-llm-models:/root/.ollama
      - ./services/llm-service/healthcheck.sh:/healthcheck.sh:ro
      - ./data/models:/host-models:ro
    runtime: nvidia
    deploy:
      resources:
        limits:
          cpus: '${CPU_LIMIT_LLM}'
          memory: ${RAM_LIMIT_LLM}
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      postgres-db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "/bin/bash", "/healthcheck.sh"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 300s
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "10"

  # 5. Embedding Service
  embedding-service:
    build:
      context: ./services/embedding-service
      dockerfile: Dockerfile
    container_name: embedding-service
    hostname: embedding-service
    restart: always
    networks:
      - arasul-net
    environment:
      MODEL_NAME: ${EMBEDDING_MODEL}
      SERVICE_PORT: ${EMBEDDING_SERVICE_PORT}
      SERVICE_URL: http://localhost:${EMBEDDING_SERVICE_PORT}
      VECTOR_SIZE: ${EMBEDDING_VECTOR_SIZE}
      MAX_INPUT_TOKENS: ${EMBEDDING_MAX_INPUT_TOKENS}
      # Jetson Orin GPU activation
      CUDA_VISIBLE_DEVICES: 0
      TORCH_CUDA_ARCH_LIST: "8.7"
    volumes:
      - arasul-embeddings-models:/models
      - ./services/embedding-service/healthcheck.sh:/healthcheck.sh:ro
    runtime: nvidia
    deploy:
      resources:
        limits:
          cpus: '${CPU_LIMIT_EMBEDDING}'
          memory: ${RAM_LIMIT_EMBEDDING}
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      postgres-db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "/bin/bash", "/healthcheck.sh"]
      interval: 15s
      timeout: 3s
      retries: 3
      start_period: 300s
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "10"

  # 5.5 Document Indexer - Automatic document indexing for RAG with AI analysis
  document-indexer:
    build:
      context: ./services/document-indexer
      dockerfile: Dockerfile
    container_name: document-indexer
    hostname: document-indexer
    restart: always
    networks:
      - arasul-net
    environment:
      # MinIO Configuration
      MINIO_HOST: ${MINIO_HOST}
      MINIO_PORT: ${MINIO_PORT}
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      DOCUMENT_INDEXER_MINIO_BUCKET: ${DOCUMENT_INDEXER_MINIO_BUCKET}
      # Qdrant Configuration
      QDRANT_HOST: ${QDRANT_HOST}
      QDRANT_PORT: ${QDRANT_PORT}
      QDRANT_COLLECTION_NAME: ${QDRANT_COLLECTION_NAME}
      # Embedding Service
      EMBEDDING_SERVICE_HOST: ${EMBEDDING_SERVICE_HOST}
      EMBEDDING_SERVICE_PORT: ${EMBEDDING_SERVICE_PORT}
      EMBEDDING_VECTOR_SIZE: ${EMBEDDING_VECTOR_SIZE}
      EMBEDDING_MODEL: ${EMBEDDING_MODEL}
      # LLM Service (for AI analysis)
      LLM_SERVICE_HOST: ${LLM_SERVICE_HOST}
      LLM_SERVICE_PORT: ${LLM_SERVICE_PORT}
      LLM_MODEL: ${LLM_MODEL}
      # PostgreSQL (for metadata storage)
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PORT: ${POSTGRES_PORT}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      # Indexer Configuration
      DOCUMENT_INDEXER_INTERVAL: ${DOCUMENT_INDEXER_INTERVAL}
      DOCUMENT_INDEXER_CHUNK_SIZE: ${DOCUMENT_INDEXER_CHUNK_SIZE}
      DOCUMENT_INDEXER_CHUNK_OVERLAP: ${DOCUMENT_INDEXER_CHUNK_OVERLAP}
      DOCUMENT_INDEXER_API_PORT: ${DOCUMENT_INDEXER_API_PORT:-9102}
      # AI Features
      DOCUMENT_INDEXER_ENABLE_AI: ${DOCUMENT_INDEXER_ENABLE_AI:-true}
      DOCUMENT_INDEXER_ENABLE_SIMILARITY: ${DOCUMENT_INDEXER_ENABLE_SIMILARITY:-true}
      DOCUMENT_INDEXER_SIMILARITY_THRESHOLD: ${DOCUMENT_INDEXER_SIMILARITY_THRESHOLD:-0.8}
      # CRITICAL-FIX: Maximum file size to prevent OOM (default 100MB)
      DOCUMENT_MAX_SIZE_MB: ${DOCUMENT_MAX_SIZE_MB:-100}
    depends_on:
      postgres-db:
        condition: service_healthy
      minio:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      embedding-service:
        condition: service_healthy
      llm-service:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9102/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 60s
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "10"
    # PHASE2-FIX: Add memory limits
    deploy:
      resources:
        limits:
          memory: 2G

  # 6. Reverse Proxy (Traefik) - Starts AFTER core services, BEFORE dashboards
  reverse-proxy:
    image: traefik:v2.11
    container_name: reverse-proxy
    hostname: reverse-proxy
    restart: always
    networks:
      - arasul-net
    command:
      - "--configFile=/etc/traefik/traefik.yml"
    ports:
      - "80:80"
      - "443:443"
      - "127.0.0.1:8080:8080"  # Dashboard only on localhost
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./config/traefik:/etc/traefik:ro
      - arasul-letsencrypt:/letsencrypt
      - ./logs:/arasul/logs
    # HIGH-014 FIX: Ensure all backend services are healthy before starting reverse proxy
    depends_on:
      postgres-db:
        condition: service_healthy
      minio:
        condition: service_healthy
      metrics-collector:
        condition: service_healthy
      llm-service:
        condition: service_healthy
      embedding-service:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "traefik", "healthcheck", "--ping"]
      interval: 10s
      timeout: 3s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "10"
    # PHASE2-FIX: Add memory limits
    deploy:
      resources:
        limits:
          memory: 512M

  # 7. Dashboard Backend
  dashboard-backend:
    build:
      context: ./services/dashboard-backend
      dockerfile: Dockerfile
    container_name: dashboard-backend
    hostname: dashboard-backend
    restart: always
    networks:
      - arasul-net
    # Add docker group (994) to allow access to docker.sock for service status monitoring
    group_add:
      - "994"
    environment:
      NODE_ENV: production
      PORT: ${DASHBOARD_BACKEND_PORT}
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PORT: ${POSTGRES_PORT}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      LLM_SERVICE_HOST: ${LLM_SERVICE_HOST}
      LLM_SERVICE_PORT: ${LLM_SERVICE_PORT}
      EMBEDDING_SERVICE_HOST: ${EMBEDDING_SERVICE_HOST}
      EMBEDDING_SERVICE_PORT: ${EMBEDDING_SERVICE_PORT}
      MINIO_HOST: ${MINIO_HOST}
      MINIO_PORT: ${MINIO_PORT}
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      N8N_HOST: ${N8N_HOST}
      N8N_PORT: ${N8N_PORT}
      QDRANT_HOST: ${QDRANT_HOST}
      QDRANT_PORT: ${QDRANT_PORT}
      QDRANT_COLLECTION_NAME: ${QDRANT_COLLECTION_NAME}
      DOCUMENT_INDEXER_HOST: ${DOCUMENT_INDEXER_HOST:-document-indexer}
      DOCUMENT_INDEXER_API_PORT: ${DOCUMENT_INDEXER_API_PORT:-9102}
      DOCUMENT_INDEXER_MINIO_BUCKET: ${DOCUMENT_INDEXER_MINIO_BUCKET:-documents}
      JWT_SECRET: ${JWT_SECRET}
      JWT_EXPIRY: ${JWT_EXPIRY}
      ADMIN_USERNAME: ${ADMIN_USERNAME}
      SYSTEM_VERSION: ${SYSTEM_VERSION}
      BUILD_HASH: ${BUILD_HASH}
      LOG_LEVEL: ${LOG_LEVEL}
      ENV_FILE_PATH: /arasul/config/.env
      COMPOSE_PROJECT_DIR: /home/arasul/arasul/arasul-jet
    volumes:
      # PHASE1-FIX: Docker socket requires write access for app management (create/start/stop containers)
      # Security note: This grants container full Docker control - minimize other privileges
      - /var/run/docker.sock:/var/run/docker.sock
      - ./.env:/arasul/config/.env
      - ./config:/config:ro
      - ./data/updates:/arasul/updates
      - ./data/backups:/arasul/backups:ro
      - ./data/appstore/manifests:/arasul/appstore/manifests:ro
      - ./data/ssh-keys:/arasul/ssh-keys:ro
    depends_on:
      postgres-db:
        condition: service_healthy
      minio:
        condition: service_healthy
      metrics-collector:
        condition: service_healthy
      llm-service:
        condition: service_healthy
      embedding-service:
        condition: service_healthy
      reverse-proxy:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://127.0.0.1:3001/api/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '${CPU_LIMIT_DASHBOARD}'
    # FIX: Routing komplett 端ber File Provider (config/traefik/dynamic/routes.yml)
    # Docker-Labels deaktiviert, da sie nur websecure (HTTPS) unterst端tzten
    # und HTTP-Zugriff (localhost, LAN) blockierten
    labels:
      - "traefik.enable=false"
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "10"

  # 8. Dashboard Frontend
  dashboard-frontend:
    build:
      context: ./services/dashboard-frontend
      dockerfile: Dockerfile
    container_name: dashboard-frontend
    hostname: dashboard-frontend
    restart: always
    networks:
      - arasul-net
    # Note: React ENV vars are embedded at BUILD time, not runtime
    # Frontend uses dynamic detection from window.location
    depends_on:
      reverse-proxy:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "test", "-f", "/usr/share/nginx/html/index.html"]
      interval: 10s
      timeout: 1s
      retries: 3
    # FIX: Routing komplett 端ber File Provider (config/traefik/dynamic/routes.yml)
    # Docker-Labels deaktiviert, da sie nur websecure (HTTPS) unterst端tzten
    # und HTTP-Zugriff (localhost, LAN) blockierten
    labels:
      - "traefik.enable=false"
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "10"
    # PHASE2-FIX: Add memory limits
    deploy:
      resources:
        limits:
          memory: 256M

  # 9. n8n Workflow Engine
  n8n:
    build:
      context: ./services/n8n
      dockerfile: Dockerfile
    container_name: n8n
    hostname: n8n
    restart: always
    networks:
      - arasul-net
    ports:
      - "5678:5678"
    environment:
      N8N_HOST: ${N8N_HOST}
      N8N_PORT: ${N8N_PORT}
      N8N_PROTOCOL: ${N8N_PROTOCOL:-http}
      # PHASE2-FIX: Make SECURE_COOKIE configurable (default false for LAN access)
      N8N_SECURE_COOKIE: "${N8N_SECURE_COOKIE:-false}"
      # N8N_EDITOR_BASE_URL tells n8n the external URL (for links in emails, etc.)
      # Traefik strips /n8n prefix before forwarding to n8n
      N8N_EDITOR_BASE_URL: http://${INTERNAL_IP}/n8n
      # Disable basic auth - use n8n's built-in user management instead
      N8N_BASIC_AUTH_ACTIVE: "false"
      WEBHOOK_URL: http://${N8N_HOST}:${N8N_WEBHOOK_PORT}
      N8N_ENCRYPTION_KEY: ${N8N_ENCRYPTION_KEY}
      EXECUTIONS_DATA_SAVE_ON_SUCCESS: all
      EXECUTIONS_DATA_SAVE_ON_ERROR: all
      EXECUTIONS_DATA_SAVE_MANUAL_EXECUTIONS: "true"
      # Session duration: 30 days (720 hours) for persistent login
      N8N_USER_MANAGEMENT_JWT_DURATION_HOURS: "720"
      # Enable personalization for better UX
      N8N_PERSONALIZATION_ENABLED: "true"
      # Use WebSocket for real-time updates
      N8N_PUSH_BACKEND: "websocket"
      # Timezone
      GENERIC_TIMEZONE: "Europe/Berlin"
      TZ: "Europe/Berlin"
      # Custom nodes are baked into the image via Dockerfile
      N8N_CUSTOM_EXTENSIONS: /custom-nodes
      # n8n 2.x: Trust proxy for correct X-Forwarded-For handling (behind Traefik)
      N8N_TRUST_PROXY: "true"
      # n8n 2.x: Enable task runners for secure code execution (sandbox)
      N8N_RUNNERS_ENABLED: "true"
      N8N_RUNNERS_MODE: "internal"
    volumes:
      - arasul-n8n:/home/node/.n8n
      # Mount credential templates
      - ./services/n8n/credentials:/custom-credentials:ro
      # Mount workflow templates
      - ./services/n8n/templates:/custom-templates:ro
    depends_on:
      postgres-db:
        condition: service_healthy
      llm-service:
        condition: service_healthy
      embedding-service:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:5678/healthz"]
      interval: 15s
      timeout: 2s
      retries: 3
    deploy:
      resources:
        limits:
          memory: ${RAM_LIMIT_N8N}
    labels:
      # Router defined in config/traefik/dynamic/routes.yml (n8n handles own auth)
      - "traefik.enable=true"
      - "traefik.http.services.n8n-docker.loadbalancer.server.port=5678"
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "10"

  # 10. Self-Healing Engine
  self-healing-agent:
    build:
      context: ./services/self-healing-agent
      dockerfile: Dockerfile
    container_name: self-healing-agent
    hostname: self-healing-agent
    restart: always
    networks:
      - arasul-net
    environment:
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PORT: ${POSTGRES_PORT}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      SELF_HEALING_INTERVAL: ${SELF_HEALING_INTERVAL}
      SELF_HEALING_ENABLED: ${SELF_HEALING_ENABLED}
      LOG_LEVEL: ${SELF_HEALING_LOG_LEVEL}
      METRICS_COLLECTOR_HOST: metrics-collector
      METRICS_COLLECTOR_PORT: 9100
      DISK_WARNING_PERCENT: ${DISK_WARNING_PERCENT}
      DISK_CLEANUP_PERCENT: ${DISK_CLEANUP_PERCENT}
      DISK_CRITICAL_PERCENT: ${DISK_CRITICAL_PERCENT}
      DISK_REBOOT_PERCENT: ${DISK_REBOOT_PERCENT}
      HEARTBEAT_PORT: ${SELF_HEALING_HEARTBEAT_PORT:-9200}
    volumes:
      # PHASE1-FIX: Docker socket requires write access for container restart/recovery
      # Security note: Combined with CAP_SYS_ADMIN - ensure process isolation
      - /var/run/docker.sock:/var/run/docker.sock
      - ./logs:/arasul/logs
      - /sys:/host/sys:ro
      - /proc:/host/proc:ro
      - /media:/media:ro
      - /mnt:/mnt:ro
      - ./data/updates:/arasul/updates
      - ./data/backups:/arasul/backups
    devices:
      - /dev/bus/usb:/dev/bus/usb
    cap_add:
      - SYS_ADMIN   # Required for system management operations
      - SYS_BOOT    # Required for reboot capability
      - SYS_NICE    # Required for process priority management
    depends_on:
      postgres-db:
        condition: service_healthy
      metrics-collector:
        condition: service_healthy
      dashboard-backend:
        condition: service_healthy
      llm-service:
        condition: service_healthy
      embedding-service:
        condition: service_healthy
      n8n:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python3", "/app/heartbeat.py", "--test"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 10s
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "10"
    # PHASE2-FIX: Add memory limits
    deploy:
      resources:
        limits:
          memory: 512M

  # 11. Backup Service - Automated PostgreSQL and MinIO backups
  backup-service:
    image: alpine:3.19
    container_name: backup-service
    hostname: backup-service
    restart: unless-stopped
    networks:
      - arasul-net
    environment:
      POSTGRES_HOST: postgres-db
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      MINIO_HOST: minio
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      BACKUP_SCHEDULE: ${BACKUP_SCHEDULE:-0 2 * * *}
      BACKUP_RETENTION_DAYS: ${BACKUP_RETENTION_DAYS:-30}
      TZ: ${TZ:-Europe/Berlin}
    volumes:
      - ./data/backups:/backups
      - /var/run/docker.sock:/var/run/docker.sock:ro
    entrypoint: /bin/sh
    command:
      - -c
      - |
        # Install required packages
        apk add --no-cache docker-cli postgresql16-client gzip tar curl bash

        # Create backup script
        cat > /usr/local/bin/backup.sh << 'BACKUP_SCRIPT'
        #!/bin/bash
        set -e
        TIMESTAMP=$$(date +%Y%m%d_%H%M%S)
        echo "[$$TIMESTAMP] Starting backup..."

        # PostgreSQL backup
        mkdir -p /backups/postgres
        PGPASSWORD=$$POSTGRES_PASSWORD pg_dump \
          -h $$POSTGRES_HOST \
          -U $$POSTGRES_USER \
          -d $$POSTGRES_DB \
          --no-owner --no-acl --clean --if-exists \
          | gzip > /backups/postgres/arasul_db_$$TIMESTAMP.sql.gz
        ln -sf arasul_db_$$TIMESTAMP.sql.gz /backups/postgres/arasul_db_latest.sql.gz
        echo "[$$TIMESTAMP] PostgreSQL backup completed"

        # MinIO backup via docker exec
        mkdir -p /backups/minio
        docker exec minio mc alias set local http://localhost:9000 $$MINIO_ROOT_USER $$MINIO_ROOT_PASSWORD 2>/dev/null || true
        docker exec minio mc mirror --overwrite local/documents /tmp/backup_docs 2>/dev/null || true
        docker cp minio:/tmp/backup_docs /tmp/minio_backup_$$TIMESTAMP 2>/dev/null || mkdir -p /tmp/minio_backup_$$TIMESTAMP
        tar -czf /backups/minio/documents_$$TIMESTAMP.tar.gz -C /tmp minio_backup_$$TIMESTAMP 2>/dev/null || true
        ln -sf documents_$$TIMESTAMP.tar.gz /backups/minio/documents_latest.tar.gz
        rm -rf /tmp/minio_backup_$$TIMESTAMP
        docker exec minio rm -rf /tmp/backup_docs 2>/dev/null || true
        echo "[$$TIMESTAMP] MinIO backup completed"

        # Cleanup old backups
        find /backups/postgres -name "*.sql.gz" ! -name "*latest*" -mtime +$$BACKUP_RETENTION_DAYS -delete 2>/dev/null || true
        find /backups/minio -name "*.tar.gz" ! -name "*latest*" -mtime +$$BACKUP_RETENTION_DAYS -delete 2>/dev/null || true
        echo "[$$TIMESTAMP] Cleanup completed"

        # Generate report
        cat > /backups/backup_report.json << EOF
        {
          "timestamp": "$$(date -Iseconds)",
          "status": "completed",
          "postgres_backups": $$(ls /backups/postgres/*.sql.gz 2>/dev/null | wc -l),
          "minio_backups": $$(ls /backups/minio/*.tar.gz 2>/dev/null | wc -l),
          "retention_days": $$BACKUP_RETENTION_DAYS
        }
        EOF
        echo "[$$TIMESTAMP] Backup completed successfully"
        BACKUP_SCRIPT
        chmod +x /usr/local/bin/backup.sh

        # Run initial backup
        echo "Running initial backup..."
        /usr/local/bin/backup.sh || echo "Initial backup skipped (services may not be ready)"

        # Setup cron
        echo "$$BACKUP_SCHEDULE /usr/local/bin/backup.sh >> /backups/backup.log 2>&1" > /etc/crontabs/root
        echo "Backup service started. Schedule: $$BACKUP_SCHEDULE"

        # Run crond in foreground
        crond -f -l 2
    depends_on:
      postgres-db:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "test", "-f", "/backups/backup_report.json"]
      interval: 60s
      timeout: 5s
      retries: 3
      start_period: 120s
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"
    # PHASE2-FIX: Add memory limits
    deploy:
      resources:
        limits:
          memory: 256M

  # 12. Telegram Bot - System notifications and command interface
  telegram-bot:
    build:
      context: ./services/telegram-bot
      dockerfile: Dockerfile
    container_name: telegram-bot
    hostname: telegram-bot
    restart: always
    networks:
      - arasul-net
    environment:
      # Telegram Configuration
      TELEGRAM_BOT_TOKEN: ${TELEGRAM_BOT_TOKEN}
      TELEGRAM_CHAT_ID: ${TELEGRAM_CHAT_ID:-}
      TELEGRAM_ALLOWED_USERS: ${TELEGRAM_ALLOWED_USERS:-}
      TELEGRAM_BOT_PORT: ${TELEGRAM_BOT_PORT:-8090}
      # Feature Flags
      TELEGRAM_NOTIFY_STARTUP: ${TELEGRAM_NOTIFY_STARTUP:-true}
      TELEGRAM_NOTIFY_ERRORS: ${TELEGRAM_NOTIFY_ERRORS:-true}
      # Backend Integration
      DASHBOARD_BACKEND_URL: http://dashboard-backend:${DASHBOARD_BACKEND_PORT:-3001}
      # Logging
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
    depends_on:
      postgres-db:
        condition: service_healthy
      dashboard-backend:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8090/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 10s
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "10"
    deploy:
      resources:
        limits:
          memory: 256M
